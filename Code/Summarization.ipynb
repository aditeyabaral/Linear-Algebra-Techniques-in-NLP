{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "1\n",
      "(0.81742320293467, 0.3959564279913752)\n",
      "2\n",
      "(0.8309963597141702, 0.4617769987865714)\n",
      "3\n",
      "(0.7098817567567568, 0.6013513513513513)\n",
      "4\n",
      "(0.749056904521545, 0.5329153605015674)\n",
      "5\n",
      "(0.8242067553735927, 0.42067553735926305)\n",
      "6\n",
      "(0.8456375838926175, 0.46721734641197726)\n",
      "7\n",
      "(0.7039366663051729, 0.5702201496583884)\n",
      "8\n",
      "(0.8434237995824635, 0.49478079331941544)\n",
      "9\n",
      "(0.7991037376048817, 0.42581998474446986)\n"
     ]
    }
   ],
   "source": [
    "from TextPreprocessing import clean\n",
    "import SummarizationLSA\n",
    "import SummarizationTFIDF\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rake_nltk import Rake\n",
    "from nltk.book import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "documents = [text1,text2,text3,text4,text5,text6,text7,text8,text9]\n",
    "documents = [\" \".join(i.tokens) for i in documents]\n",
    "\n",
    "r = Rake()\n",
    "keyvalues = []\n",
    "ctr = 1\n",
    "for doc in documents:\n",
    "    print(ctr)\n",
    "    r.extract_keywords_from_text(doc)\n",
    "    rake_count_total = len(r.get_ranked_phrases())\n",
    "    \n",
    "    X = sent_tokenize(doc)\n",
    "    X_clean = list(map(clean,X))\n",
    "    \n",
    "    sentence_scores = SummarizationTFIDF.sentence_score(X_clean)\n",
    "    summary1 = SummarizationTFIDF.summarize_avg(X,sentence_scores)\n",
    "    r.extract_keywords_from_text(summary1)\n",
    "    rake_count_tfidf = len(r.get_ranked_phrases())\n",
    "    \n",
    "    k = 200\n",
    "    new_k, Sigma, lsa, document_topic_matrix, term_topic_matrix = SummarizationLSA.LSA(k,X)\n",
    "    percentage_topic = SummarizationLSA.percent_sigma(Sigma,X).astype(int)\n",
    "    summary2 = SummarizationLSA.summarize(new_k,percentage_topic,document_topic_matrix)\n",
    "    r.extract_keywords_from_text(summary2)\n",
    "    rake_count_lsa = len(r.get_ranked_phrases())\n",
    "    \n",
    "    t = (rake_count_tfidf/rake_count_total,rake_count_lsa/rake_count_total)\n",
    "    print(t)\n",
    "    keyvalues.append(t)\n",
    "    ctr+=1\n",
    "    \n",
    "df = pd.DataFrame(keyvalues,columns = [\"TF-IDF\",\"LSA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
